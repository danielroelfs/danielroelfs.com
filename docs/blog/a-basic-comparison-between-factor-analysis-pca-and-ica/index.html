<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Daniel Roelfs">
    <meta name="description" content="/">
    <meta name="keywords" content="blog,personal,coding">

    <meta property="og:site_name" content="Daniel Roelfs">
    <meta property="og:title" content="
  A Basic Comparison Between Factor Analysis, PCA, and ICA - Daniel Roelfs
">
    <meta property="og:description" content="A Basic Comparison Between Factor Analysis, PCA, and ICA">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/blog/a-basic-comparison-between-factor-analysis-pca-and-ica/">
    <meta property="og:image" content="/images/avatar.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="/blog/a-basic-comparison-between-factor-analysis-pca-and-ica/">
    <meta name="twitter:image" content="/images/avatar.png">

    <base href="/blog/a-basic-comparison-between-factor-analysis-pca-and-ica/">
    <title>
  A Basic Comparison Between Factor Analysis, PCA, and ICA - Daniel Roelfs
</title>

    <link rel="canonical" href="/blog/a-basic-comparison-between-factor-analysis-pca-and-ica/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
    
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Alegreya:ital,wght@0,400;0,700;1,400;1,700&family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=Nunito+Sans:ital,wght@0,400;0,700;0,800;0,900;1,400;1,700;1,800;1,900&display=swap" rel="stylesheet">    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Daniel Roelfs">
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="Daniel Roelfs" />
    

    
      <script async defer data-website-id="37592e98-527a-4a7d-9154-5ba0be3a4684" src="https://analytics-danielroelfs.herokuapp.com/umami.js"></script>
    

    <meta name="generator" content="Hugo 0.92.1" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Daniel Roelfs</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/blog">Blog</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/photography">Photography</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/publications">Publications</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/cv">Curriculum Vitæ</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/about">About</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="post-title">A Basic Comparison Between Factor Analysis, PCA, and ICA</h1> 
      <h2 class="date">September 14, 2021</h2>

      
    </header>

    <p>This is just a very quick blog post outlining some of the commonalities
and differences between factor analysis (FA), principal component
analysis (PCA), and independent component analysis (ICA). I was inspired
to write some of this down through some confusion caused in the lab by
SPSS' apparent dual usage of the term &ldquo;factor analysis&rdquo; and &ldquo;principal
components&rdquo;. A few of my colleagues who use SPSS showed me the following
screen:</p>
<p align="center">
<img src="spss-screenshot.png"/>
</p>
<p>This screen shows up when you click <code>Analyze</code> -&gt; <code>Dimension Reduction</code>
-&gt; <code>Factor</code>, which then opens a window called &ldquo;Factor Analysis:
Extraction&rdquo; which lets you pick &ldquo;Principal components&rdquo; as a method. To
put the (apparent) PCA method as a sub-section of factor analysis is
misleading at best, and straight-up erronious at worst. The other
options for the method here is &ldquo;Principal axis factoring&rdquo; (which is
closer to traditional factor analysis) and &ldquo;Maximum likelihood&rdquo; (<a href="https://stats.idre.ucla.edu/spss/seminars/efa-spss/">source
for screenshot and SPSS
interface</a>). If
you&rsquo;re wondering if you&rsquo;re the first one to be confused by SPSS' choice
to present this in such a way, you&rsquo;re not
(<a href="https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi">link</a>,
<a href="https://stats.stackexchange.com/questions/24781/interpreting-discrepancies-between-r-and-spss-with-exploratory-factor-analysis">link</a>).</p>
<p>Standard disclaimer: I&rsquo;m not a statistician, and I&rsquo;m definitely not
confident enough to go in-depth into the mathematics of the different
algorithms. Instead, I&rsquo;ll just run three common latent variable
modeling/clustering methods, and show the difference in results when
applied to the same data. Where-ever I feel confident, I will also
elaborate on the underlying mathematical principles and concepts. It&rsquo;s a
short post, and I&rsquo;m sure there&rsquo;s levels of nuance and complexity I&rsquo;ve
missed. Please let me know if you think I&rsquo;ve committed a major
oversight.</p>
<p>To show the methods I&rsquo;ll use a small dataset on cervical cancer risk
factors with only 72 entries and 20 variables. This dataset I downloaded
from the <a href="https://archive.ics.uci.edu/ml/datasets/Cervical+Cancer+Behavior+Risk#">UCI Machine Learning
Repository</a>
and was based on <a href="https://doi.org/10.1166/asl.2016.7980">this paper</a> by
Sobar <em>et al</em>. The dataset consists of 19 risk factors for cervical
cancer, and 1 outcome variable (binary outcome variable with yes/no
cervical cancer).</p>
<p>We&rsquo;ll load a few packages first, <code>{tidyverse}</code> for data wrangling and
visualization, <code>{fastICA}</code> to get access to an ICA algorithm, and
<code>{patchwork}</code> to combine different plots into one.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="nf">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">fastICA</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">patchwork</span><span class="p">)</span>
</code></pre></div><p>We&rsquo;ll load the data into R, clean up the variable names, convert the
outcome variable (<code>ca_cervix</code>) to a factor. We&rsquo;ll have a look at the
dataset using some functions from <code>{skimr}</code>. This function will give us
summary statistics and basic histograms of the different variables.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">data</span> <span class="o">&lt;-</span> <span class="nf">read_csv</span><span class="p">(</span><span class="s">&#34;sobar-72.csv&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="n">janitor</span><span class="o">::</span><span class="nf">clean_names</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="n">ca_cervix</span> <span class="o">=</span> <span class="nf">as_factor</span><span class="p">(</span><span class="n">ca_cervix</span><span class="p">))</span>

<span class="n">skim_summ</span> <span class="o">&lt;-</span> <span class="n">skimr</span><span class="o">::</span><span class="nf">skim_with</span><span class="p">(</span><span class="n">base</span> <span class="o">=</span> <span class="n">skimr</span><span class="o">::</span><span class="nf">sfl</span><span class="p">())</span>
<span class="nf">skim_summ</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Name</td>
<td style="text-align:left">data</td>
</tr>
<tr>
<td style="text-align:left">Number of rows</td>
<td style="text-align:left">72</td>
</tr>
<tr>
<td style="text-align:left">Number of columns</td>
<td style="text-align:left">20</td>
</tr>
<tr>
<td style="text-align:left">_______________________</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Column type frequency:</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">factor</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">numeric</td>
<td style="text-align:left">19</td>
</tr>
<tr>
<td style="text-align:left">________________________</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Group variables</td>
<td style="text-align:left">None</td>
</tr>
</tbody>
</table>
<p>Data summary</p>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">skim_variable</th>
<th style="text-align:left">ordered</th>
<th style="text-align:right">n_unique</th>
<th style="text-align:left">top_counts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ca_cervix</td>
<td style="text-align:left">FALSE</td>
<td style="text-align:right">2</td>
<td style="text-align:left">0: 51, 1: 21</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">skim_variable</th>
<th style="text-align:right">mean</th>
<th style="text-align:right">sd</th>
<th style="text-align:right">p0</th>
<th style="text-align:right">p25</th>
<th style="text-align:right">p50</th>
<th style="text-align:right">p75</th>
<th style="text-align:right">p100</th>
<th style="text-align:left">hist</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">behavior_sexual_risk</td>
<td style="text-align:right">9.67</td>
<td style="text-align:right">1.19</td>
<td style="text-align:right">2</td>
<td style="text-align:right">10.00</td>
<td style="text-align:right">10.0</td>
<td style="text-align:right">10.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▁▁▁▁▇</td>
</tr>
<tr>
<td style="text-align:left">behavior_eating</td>
<td style="text-align:right">12.79</td>
<td style="text-align:right">2.36</td>
<td style="text-align:right">3</td>
<td style="text-align:right">11.00</td>
<td style="text-align:right">13.0</td>
<td style="text-align:right">15.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▁▁▂▅▇</td>
</tr>
<tr>
<td style="text-align:left">behavior_personal_hygine</td>
<td style="text-align:right">11.08</td>
<td style="text-align:right">3.03</td>
<td style="text-align:right">3</td>
<td style="text-align:right">9.00</td>
<td style="text-align:right">11.0</td>
<td style="text-align:right">14.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▁▂▆▆▇</td>
</tr>
<tr>
<td style="text-align:left">intention_aggregation</td>
<td style="text-align:right">7.90</td>
<td style="text-align:right">2.74</td>
<td style="text-align:right">2</td>
<td style="text-align:right">6.00</td>
<td style="text-align:right">10.0</td>
<td style="text-align:right">10.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▂▁▂▁▇</td>
</tr>
<tr>
<td style="text-align:left">intention_commitment</td>
<td style="text-align:right">13.35</td>
<td style="text-align:right">2.37</td>
<td style="text-align:right">6</td>
<td style="text-align:right">11.00</td>
<td style="text-align:right">15.0</td>
<td style="text-align:right">15.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▁▁▂▁▇</td>
</tr>
<tr>
<td style="text-align:left">attitude_consistency</td>
<td style="text-align:right">7.18</td>
<td style="text-align:right">1.52</td>
<td style="text-align:right">2</td>
<td style="text-align:right">6.00</td>
<td style="text-align:right">7.0</td>
<td style="text-align:right">8.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▁▁▃▇▂</td>
</tr>
<tr>
<td style="text-align:left">attitude_spontaneity</td>
<td style="text-align:right">8.61</td>
<td style="text-align:right">1.52</td>
<td style="text-align:right">4</td>
<td style="text-align:right">8.00</td>
<td style="text-align:right">9.0</td>
<td style="text-align:right">10.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▁▂▁▅▇</td>
</tr>
<tr>
<td style="text-align:left">norm_significant_person</td>
<td style="text-align:right">3.12</td>
<td style="text-align:right">1.85</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1.00</td>
<td style="text-align:right">3.0</td>
<td style="text-align:right">5.00</td>
<td style="text-align:right">5</td>
<td style="text-align:left">▇▁▂▁▇</td>
</tr>
<tr>
<td style="text-align:left">norm_fulfillment</td>
<td style="text-align:right">8.49</td>
<td style="text-align:right">4.91</td>
<td style="text-align:right">3</td>
<td style="text-align:right">3.00</td>
<td style="text-align:right">7.0</td>
<td style="text-align:right">14.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▇▁▁▂▆</td>
</tr>
<tr>
<td style="text-align:left">perception_vulnerability</td>
<td style="text-align:right">8.51</td>
<td style="text-align:right">4.28</td>
<td style="text-align:right">3</td>
<td style="text-align:right">5.00</td>
<td style="text-align:right">8.0</td>
<td style="text-align:right">13.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▇▅▅▂▇</td>
</tr>
<tr>
<td style="text-align:left">perception_severity</td>
<td style="text-align:right">5.39</td>
<td style="text-align:right">3.40</td>
<td style="text-align:right">2</td>
<td style="text-align:right">2.00</td>
<td style="text-align:right">4.0</td>
<td style="text-align:right">9.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▇▂▁▂▅</td>
</tr>
<tr>
<td style="text-align:left">motivation_strength</td>
<td style="text-align:right">12.65</td>
<td style="text-align:right">3.21</td>
<td style="text-align:right">3</td>
<td style="text-align:right">11.00</td>
<td style="text-align:right">14.0</td>
<td style="text-align:right">15.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▁▁▁▂▇</td>
</tr>
<tr>
<td style="text-align:left">motivation_willingness</td>
<td style="text-align:right">9.69</td>
<td style="text-align:right">4.13</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7.00</td>
<td style="text-align:right">11.0</td>
<td style="text-align:right">13.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▅▃▃▃▇</td>
</tr>
<tr>
<td style="text-align:left">social_support_emotionality</td>
<td style="text-align:right">8.10</td>
<td style="text-align:right">4.24</td>
<td style="text-align:right">3</td>
<td style="text-align:right">3.00</td>
<td style="text-align:right">9.0</td>
<td style="text-align:right">11.25</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▇▂▃▃▅</td>
</tr>
<tr>
<td style="text-align:left">social_support_appreciation</td>
<td style="text-align:right">6.17</td>
<td style="text-align:right">2.90</td>
<td style="text-align:right">2</td>
<td style="text-align:right">3.75</td>
<td style="text-align:right">6.5</td>
<td style="text-align:right">9.00</td>
<td style="text-align:right">10</td>
<td style="text-align:left">▇▃▅▇▇</td>
</tr>
<tr>
<td style="text-align:left">social_support_instrumental</td>
<td style="text-align:right">10.38</td>
<td style="text-align:right">4.32</td>
<td style="text-align:right">3</td>
<td style="text-align:right">6.75</td>
<td style="text-align:right">12.0</td>
<td style="text-align:right">14.25</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▅▂▂▃▇</td>
</tr>
<tr>
<td style="text-align:left">empowerment_knowledge</td>
<td style="text-align:right">10.54</td>
<td style="text-align:right">4.37</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7.00</td>
<td style="text-align:right">12.0</td>
<td style="text-align:right">15.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▃▁▂▂▇</td>
</tr>
<tr>
<td style="text-align:left">empowerment_abilities</td>
<td style="text-align:right">9.32</td>
<td style="text-align:right">4.18</td>
<td style="text-align:right">3</td>
<td style="text-align:right">5.00</td>
<td style="text-align:right">10.0</td>
<td style="text-align:right">13.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▇▃▅▃▇</td>
</tr>
<tr>
<td style="text-align:left">empowerment_desires</td>
<td style="text-align:right">10.28</td>
<td style="text-align:right">4.48</td>
<td style="text-align:right">3</td>
<td style="text-align:right">6.75</td>
<td style="text-align:right">11.0</td>
<td style="text-align:right">15.00</td>
<td style="text-align:right">15</td>
<td style="text-align:left">▅▁▃▃▇</td>
</tr>
</tbody>
</table>
<p>Now let&rsquo;s only select the variables containing the risk factors (called
features from here on). We&rsquo;ll also scale all the features to have a mean
of 0 and a standard deviation of 1 using the <code>scale()</code> function. We can
check what the new variable looks like using the <code>summary()</code> function.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">data_features</span> <span class="o">&lt;-</span> <span class="n">data</span> <span class="o">%&gt;%</span> 
  <span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">ca_cervix</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="nf">across</span><span class="p">(</span><span class="nf">everything</span><span class="p">(),</span> <span class="o">~</span> <span class="nf">scale</span><span class="p">(</span><span class="n">.x</span><span class="p">)))</span>

<span class="nf">summary</span><span class="p">(</span><span class="n">data_features</span><span class="o">$</span><span class="n">attitude_consistency</span><span class="p">)</span>
</code></pre></div><pre><code>       V1         
 Min.   :-3.4019  
 1st Qu.:-0.7752  
 Median :-0.1186  
 Mean   : 0.0000  
 3rd Qu.: 0.5381  
 Max.   : 1.8514  
</code></pre>
<p>So now we have a data frame with 72 entries and 19 normalized columns,
each representing a feature that may or may not predict cervical cancer.
We can create a correlation matrix to visualize the degree of
correlation between the different features. For this we simply run
<code>cor()</code> on the data frame with the features, transform the output to a
data frame in long format and then visualize it using <code>ggplot()</code> and
<code>geom_tile()</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="nf">cor</span><span class="p">(</span><span class="n">data_features</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">as_tibble</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="n">feature_y</span> <span class="o">=</span> <span class="nf">names</span><span class="p">(</span><span class="n">.)</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="n">cols</span> <span class="o">=</span> <span class="o">-</span><span class="n">feature_y</span><span class="p">,</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;feature_x&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;correlation&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="n">feature_y</span> <span class="o">=</span> <span class="nf">fct_rev</span><span class="p">(</span><span class="n">feature_y</span><span class="p">))</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">feature_x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">feature_y</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span>
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;berlin&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_fixed</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">theme</span><span class="p">(</span><span class="n">axis.text.x</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">hjust</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">angle</span> <span class="o">=</span> <span class="m">30</span><span class="p">))</span>
</code></pre></div><p><img src="index_files/figure-gfm/corr-matrix-original-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>What we&rsquo;re looking for here are little blocks along the diagonal. This
would indicate variables that correlate with each other. We can easily
spot three blocks, and we can quietly expect these variables to cluster
together later, although perhaps not necessarily in the three blocks
visible here.</p>
<p>What we showed here was the correlation matrix, if we replaced the
<code>cor()</code> function with the <code>cov()</code> function we&rsquo;d get the covariance
matrix, which would look basically identical in this case.</p>
<p>Let&rsquo;s now dive into the approaches we&rsquo;ll use. We&rsquo;ll discuss three here,
and if there&rsquo;s space, I might add a fourth one briefly. We&rsquo;ll definitely
discuss factor analysis, PCA, and ICA, and we&rsquo;ll start with the former.</p>
<h2 id="selecting-the-number-of-components">Selecting the number of components</h2>
<p>For factor analysis and ICA we need to provide the number of factors we
want to extract. We&rsquo;ll use the same number of factors/components
throughout this tutorial (also for PCA). Selection of the optimal number
of factors/components is a fairly arbitrary process which I won&rsquo;t go
into now. In short, before writing this I ran PCA and a tool called
<a href="https://research.ics.aalto.fi/ica/icasso/">icasso</a>. <em>Icasso</em> runs an
ICA algorithm a number of times and provides a number of parameters on
the stability of the clusters at different thresholds, this approach is
very data-driven. A more common and easier way to get some data-driven
insight into the optimal number of clusters is using the &ldquo;elbow&rdquo;-method
using PCA, eigenvalues of the components, and the cumulative variance
explained by the components (we&rsquo;ll show those later). However, in the
end, you should also look at the weight matrix of the different cluster
thresholds and it becomes a fairly arbitrary process. In this case, the
<em>icasso</em> showed that 7 components was good (but not the best), the
weight matrix looked okay, and 7 components explained more than 80% of
the variance in the PCA. I went for 7 components in the end also because
it served the purpose of this blogpost quite well, but I think different
thresholds are valid and you could make a case for different ones based
on the data, based on the interpretation of the weight matrix, etc. This
process is a bit of an art and a science combined.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">n_comps</span> <span class="o">&lt;-</span> <span class="m">7</span>
</code></pre></div><h2 id="factor-analysis">Factor Analysis</h2>
<p>So, let&rsquo;s run the factor analysis. Technically speaking, factor analysis
isn&rsquo;t a clustering method but rather a latent variable modeling method
<a href="https://stats.stackexchange.com/questions/241726/understanding-exploratory-factor-analysis-some-points-for-clarification">source</a>.
The primary aim of a factor analysis is the reconstruction of
correlations/covariances between variables. Maximizing explained
variance of the factors is only a secondary aim and we&rsquo;ll get to why
that is relevant in the PCA section.</p>
<p>We&rsquo;ve established we want 7 factors. The factor analysis method is
implemented in R through the <code>factanal()</code> function (see
<a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/factanal%20for%20the%20documentation">here</a>.
This function applies a common factor model using the maximum likelihood
method. We&rsquo;ll simply provide it with our data frame, the number of
factors we want to extract, and we&rsquo;ll ask to provide Bartlett&rsquo;s weighted
least-squares scores as well. We can apply a &ldquo;rotation&rdquo; to the factors
to make them reduce the complexity of the factor loadings and make them
easier to interpret, here we&rsquo;ll use the <code>varimax</code> option. Varimax
maximizes the sum of the variances of the squared loadings. Then we&rsquo;ll
print the model to see the results (and sort so the loadings are
ordered). The output may be a bit long.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">fa_model</span> <span class="o">&lt;-</span> <span class="nf">factanal</span><span class="p">(</span><span class="n">data_features</span><span class="p">,</span> <span class="n">factors</span> <span class="o">=</span> <span class="n">n_comps</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="s">&#34;Bartlett&#34;</span><span class="p">,</span> <span class="n">rotation</span> <span class="o">=</span> <span class="s">&#34;varimax&#34;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">fa_model</span><span class="p">,</span> <span class="n">sort</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</code></pre></div><pre><code>Call:
factanal(x = data_features, factors = n_comps, scores = &quot;Bartlett&quot;,     rotation = &quot;varimax&quot;)

Uniquenesses:
       behavior_sexual_risk             behavior_eating 
                      0.749                       0.005 
   behavior_personal_hygine       intention_aggregation 
                      0.287                       0.504 
       intention_commitment        attitude_consistency 
                      0.797                       0.805 
       attitude_spontaneity     norm_significant_person 
                      0.758                       0.392 
           norm_fulfillment    perception_vulnerability 
                      0.129                       0.170 
        perception_severity         motivation_strength 
                      0.143                       0.643 
     motivation_willingness social_support_emotionality 
                      0.354                       0.005 
social_support_appreciation social_support_instrumental 
                      0.005                       0.251 
      empowerment_knowledge       empowerment_abilities 
                      0.005                       0.166 
        empowerment_desires 
                      0.207 

Loadings:
                            Factor1 Factor2 Factor3 Factor4 Factor5 Factor6
motivation_willingness       0.655           0.387           0.115  -0.135 
social_support_emotionality  0.843           0.258          -0.196         
social_support_appreciation  0.849  -0.138                          -0.487 
social_support_instrumental  0.816  -0.211                   0.140         
empowerment_knowledge        0.798           0.347           0.141   0.433 
empowerment_abilities        0.861   0.118   0.188                   0.201 
empowerment_desires          0.814                           0.345         
norm_significant_person     -0.167   0.724   0.169                         
norm_fulfillment                     0.904   0.122                         
perception_vulnerability     0.124   0.879                   0.116         
perception_severity                  0.903                                 
behavior_personal_hygine     0.289   0.196   0.714   0.147  -0.154  -0.102 
intention_aggregation                        0.664           0.208         
behavior_eating                                      0.987                 
behavior_sexual_risk         0.159   0.135          -0.150   0.427         
intention_commitment                         0.174   0.111   0.394         
attitude_consistency                 0.181                  -0.103         
attitude_spontaneity                        -0.149   0.302           0.335 
motivation_strength          0.244   0.111   0.477  -0.195   0.101         
                            Factor7
motivation_willingness      -0.135 
social_support_emotionality -0.413 
social_support_appreciation        
social_support_instrumental        
empowerment_knowledge        0.172 
empowerment_abilities              
empowerment_desires                
norm_significant_person     -0.146 
norm_fulfillment             0.153 
perception_vulnerability     0.136 
perception_severity          0.153 
behavior_personal_hygine     0.162 
intention_aggregation              
behavior_eating                    
behavior_sexual_risk               
intention_commitment               
attitude_consistency         0.371 
attitude_spontaneity               
motivation_strength                

               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7
SS loadings      4.798   3.128   1.689   1.208   0.652   0.631   0.520
Proportion Var   0.253   0.165   0.089   0.064   0.034   0.033   0.027
Cumulative Var   0.253   0.417   0.506   0.570   0.604   0.637   0.665

Test of the hypothesis that 7 factors are sufficient.
The chi square statistic is 62.23 on 59 degrees of freedom.
The p-value is 0.362 
</code></pre>
<p>We may be tempted to immediately look at the <em>p</em>-value at the end of the
output. This <em>p</em>-value denotes whether the assumption of perfect fit can
be rejected. If this <em>p</em>-value is below 0.05 or 0.01 we can reject the
hypothesis of perfect fit, meaning that we could probably try a
different method or try a different number of factors. In this case, the
<em>p</em>-value is larger than 0.05 so we cannot reject the hypothesis of
perfect fit.</p>
<p>The &ldquo;Loadings&rdquo; section of the results show a make-shift weight matrix,
but in order to further interpret these results, let&rsquo;s create a plot
showing the weight matrix. We&rsquo;ll get the results from the factor
analysis model we created earlier using the <code>tidy()</code> function from
<code>{broom}</code> and convert it to long format. We&rsquo;ll then create a weight
matrix much in the same way we did earlier.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">fa_weight_matrix</span> <span class="o">&lt;-</span> <span class="n">broom</span><span class="o">::</span><span class="nf">tidy</span><span class="p">(</span><span class="n">fa_model</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="nf">starts_with</span><span class="p">(</span><span class="s">&#34;fl&#34;</span><span class="p">),</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;factor&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;loading&#34;</span><span class="p">)</span>

<span class="n">fa_loading_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">fa_weight_matrix</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">factor</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">variable</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">loading</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;FA loadings&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span> 
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;cork&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_fixed</span><span class="p">(</span><span class="n">ratio</span> <span class="o">=</span> <span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">fa_loading_plot</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/fa-weight-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>Here we can more easily see that there&rsquo;s two strong clusters in Factor 1
and Factor 2. Factor 3 captures three or four variables to a lesser
degree and Factor 4 captures mostly just a single variable
(<code>&quot;behavior_eating&quot;</code>). Interpretation of these factors is subjective, so
it is perhaps best done in collaboration with others.</p>
<p>Lastly, I think it would be interesting to see how the different factors
relate to each other. We&rsquo;ll take the Bartlett&rsquo;s scores and correlate
them with each other much like before and create a correlation matrix
like before.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">fa_model</span><span class="o">$</span><span class="n">scores</span> <span class="o">%&gt;%</span> 
  <span class="nf">cor</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">data.frame</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;factor_x&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="n">cols</span> <span class="o">=</span> <span class="o">-</span><span class="n">factor_x</span><span class="p">,</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;factor_y&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;correlation&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">factor_x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">factor_y</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span>
  <span class="nf">geom_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">correlation</span><span class="p">,</span><span class="m">4</span><span class="p">)),</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;white&#34;</span><span class="p">)</span> <span class="o">+</span>
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Correlation between FA scores&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span>
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;berlin&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_equal</span><span class="p">()</span>
</code></pre></div><p><img src="index_files/figure-gfm/fa-corr-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>We can see little correlation but certainly a few non-zero correlations
also. In particular Factor 5 and Factor 7 seem to correlate to some
extent at least and there are a few others with some minor correlations,
e.g. Factor 1 and Factor 3, as well as Factor 3 and Factor 6. We&rsquo;ll
compare this later with the other algorithms.</p>
<h2 id="principal-component-analysis">Principal Component Analysis</h2>
<p>Principal component analysis can reliably be classfied as a clustering
method (as opposed to factor analysis) and it is a very common
dimensionality reduction approach. The primary aim of PCA is to maximize
variance captured while creating uncorrelated (orthogonal) components.
Some methods associated with PCA have been borrowed from factor analysis
(e.g. scree plot, <em>Kaiser&rsquo;s rule</em>,
<a href="https://doi.org/10.1002/0470013192.bsa501">source</a>). A
<a href="https://doi.org/10.1002/0470013192.bsa501">paper</a> from professor of
Mathematics at the University of Essex Ian Jolliffe published in
<em>Encyclopedia of Statistics in Behavioral Science</em> uses fairly strong
terms to make the distinction clear:</p>
<blockquote>
<p>This is partially caused by a number of widely used software packages
treating PCA as a special case of factor analysis, which it most
certainly is not. There are several technical differences between PCA
and factor analysis, but the most fundamental difference is that
factor analysis explicitly specifies a model relating the observed
variables to a smaller set of underlying unobservable factors.
Although some authors express PCA in the framework of a model, its
main application is as a descriptive, exploratory technique, with no
thought of an underlying model.
[<a href="https://doi.org/10.1002/0470013192.bsa501">source</a>]</p>
</blockquote>
<p>I can recommend this paper as a great primer to PCA methods. It goes
over a few concepts very relevant for PCA methods as well as clustering
methods in general.</p>
<p>Now, let&rsquo;s run the PCA. In R there&rsquo;s two functions built-in to run a
principal component analysis, here we&rsquo;ll use the <code>prcomp()</code>function (the
other being <code>princomp()</code>, but <code>prcomp()</code> is preferred for reasons beyond
the scope of this post). The <code>prcomp()</code> function contains a few options
we can play with, but in my experience it works fine out of the box if
you&rsquo;ve normalized the data beforehand (<a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp">link to
documentation</a>).
So we&rsquo;ll simply provide the data frame with the features. In addition,
we&rsquo;ll also calculate the variance explained by each component. We do
that by simply taking the standard deviation calculated by the PCA and
squaring it, we&rsquo;ll save it in a new field called <code>variance</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">pc_model</span> <span class="o">&lt;-</span> <span class="nf">prcomp</span><span class="p">(</span><span class="n">data_features</span><span class="p">)</span>

<span class="n">pc_model</span><span class="o">$</span><span class="n">variance</span> <span class="o">&lt;-</span> <span class="n">pc_model</span><span class="o">$</span><span class="n">sdev^2</span>
</code></pre></div><p>Next we can make a simple scree plot using the variance we calculate
above. We&rsquo;ll create a plot with the principal components on the x-axis
and the eigenvalue on the y-axis. The scree plot is a very popular plot
to visualize features of a PCA. In this plot the elbow is quite clearly
at principal component 3, but as discussed, the scree plot is not the
best nor the only way to determine the optimal number of components. In
the code below I also added a calculation for the cumulative variance
(<code>cum_variance</code>) which showed that a little more than 80% of the
variance is captured in the first 7 components, while the first 3
components combined capture only 56%.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">pc_model</span><span class="o">$</span><span class="n">variance</span> <span class="o">%&gt;%</span> 
  <span class="nf">as_tibble</span><span class="p">()</span> <span class="o">%&gt;%</span>
  <span class="nf">rename</span><span class="p">(</span><span class="n">eigenvalue</span> <span class="o">=</span> <span class="n">value</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;comp&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="n">comp</span> <span class="o">=</span> <span class="nf">parse_number</span><span class="p">(</span><span class="n">comp</span><span class="p">),</span>
         <span class="n">cum_variance</span> <span class="o">=</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">eigenvalue</span><span class="p">)</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">eigenvalue</span><span class="p">))</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">comp</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">eigenvalue</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_hline</span><span class="p">(</span><span class="n">yintercept</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span>
  <span class="nf">geom_line</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span> 
  <span class="nf">geom_point</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="m">3</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/pca-scree-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>We&rsquo;ll also create a weight matrix again, based on the rotation from the
PCA. We&rsquo;ll create the weight matrix much in the same way as before. A
PCA by its very nature returns an equal number of components as the
number of variables put in, however, we&rsquo;re interested in just the first
7 components, so we&rsquo;ll select just those using the <code>filter()</code> function.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">pc_weight_matrix</span> <span class="o">&lt;-</span> <span class="n">pc_model</span><span class="o">$</span><span class="n">rotation</span> <span class="o">%&gt;%</span> 
  <span class="nf">data.frame</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;variable&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="nf">starts_with</span><span class="p">(</span><span class="s">&#34;PC&#34;</span><span class="p">),</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;prin_comp&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;loading&#34;</span><span class="p">)</span>

<span class="n">pca_loading_plot</span> <span class="o">&lt;-</span> <span class="n">pc_weight_matrix</span> <span class="o">%&gt;%</span> 
  <span class="nf">filter</span><span class="p">(</span><span class="nf">parse_number</span><span class="p">(</span><span class="n">prin_comp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">n_comps</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="nf">reorder</span><span class="p">(</span><span class="n">prin_comp</span><span class="p">,</span> <span class="nf">parse_number</span><span class="p">(</span><span class="n">prin_comp</span><span class="p">)),</span> <span class="n">y</span> <span class="o">=</span> <span class="n">variable</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">loading</span><span class="p">))</span> <span class="o">+</span>
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;PCA loadings&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span>
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;cork&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_fixed</span><span class="p">(</span><span class="n">ratio</span> <span class="o">=</span> <span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">pca_loading_plot</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/pca-weight-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>One thing that immediately jumps out is that PC1 and PC2 are nearly
identical to Factor 1 and Factor 2 from the factor analysis. The
direction of the sign might be reversed, but we can fix this visually by
multiplying the loadings by <code>-1</code>. The more important part though is the
other components, which look nothing like the factors from the factor
analysis. With some effort you could see some overlap between Factor 3
and PC4 as well as Factor 5 and PC 5, but both factor analysis and PCA
had different ideas about how to group the variables together other than
the first two factors/components.</p>
<p>We can also make a correlation matrix for the different principal
components. We&rsquo;ll use the <code>x</code> field and otherwise create the correlation
matrix the same way as before:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">pc_model</span><span class="o">$</span><span class="n">x</span> <span class="o">%&gt;%</span> 
  <span class="nf">cor</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">data.frame</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;comp_x&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="n">cols</span> <span class="o">=</span> <span class="nf">starts_with</span><span class="p">(</span><span class="s">&#34;PC&#34;</span><span class="p">),</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;comp_y&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;correlation&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">filter</span><span class="p">(</span><span class="nf">parse_number</span><span class="p">(</span><span class="n">comp_x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">n_comps</span><span class="p">,</span>
         <span class="nf">parse_number</span><span class="p">(</span><span class="n">comp_y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">n_comps</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">comp_x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">comp_y</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span>
  <span class="nf">geom_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">correlation</span><span class="p">,</span><span class="m">4</span><span class="p">)),</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;white&#34;</span><span class="p">)</span> <span class="o">+</span>
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Correlation between PCs&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span>
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;berlin&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_equal</span><span class="p">()</span>
</code></pre></div><p><img src="index_files/figure-gfm/pca-corr-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>What jumps out here is the apparent absence of correlation. By its
design the principal components are orthogonal to each other. The PCA
works by finding a direction (a vector) that maximizes correlation of
the features (capturing maximum variance). When it&rsquo;s done that, it moves
to the next direction, orthogonal to the previous and maximizes
correlation across that direction, and so on. Since the degree of
correlation reduces as you account for more and more of the correlations
in your data, the amount of correlation still unaccounted for reduces as
you move along the components. Now you can replace the word
&ldquo;correlation&rdquo; in these past few sentences with &ldquo;covariance&rdquo; and it would
still hold up to a large degree apart from the diagonal (again, caveats
apply).</p>
<p>Next, we can also visualize the PCA using a plot called a &ldquo;biplot&rdquo;. It
visualizes the loading of the first two components (e.g. PC1 on the
x-axis and PC2 on the y-axis) and also visualizes the direction across
which correlation was maximized. It&rsquo;s simply the <code>biplot()</code> function
implemented already in R.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="nf">biplot</span><span class="p">(</span><span class="n">pc_model</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/pca-biplot-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>It isn&rsquo;t very pretty, but we can roughly see the directions across the
first two components, one pointing straight down and one pointing to the
left.</p>
<p>A nice feature of PCA is that the underlying principles are rather
simple, and you could calculate the eigenvectors and eigenvalues using
just two functions: <code>cov()</code> and <code>eigen()</code>, which calculate the
covariance matrix and the eigenvalues and -vectors of the covariance
matrix respectively. If the terms &ldquo;matrix manipulations&rdquo;, eigenvectors,
and dot product sound scary to you, you can skip to the
<a href="#independent-component-analysis">ICA</a> section. We can then calculate
the individual loadings (stored in the <code>prcomp()</code> model in the <code>x</code>
field) by calculating the dot product of the data features and the
eigenvector. Dot product in this case means multiplying every element of
a column with the corresponding element in the eigenvector.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">cov_matrix</span> <span class="o">&lt;-</span> <span class="nf">cov</span><span class="p">(</span><span class="n">data_features</span><span class="p">)</span>

<span class="n">eigen_model</span> <span class="o">&lt;-</span> <span class="nf">eigen</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span>
<span class="n">eigenvalues</span> <span class="o">&lt;-</span> <span class="n">eigen_model</span><span class="o">$</span><span class="n">values</span>
<span class="n">eigenvector</span> <span class="o">&lt;-</span> <span class="n">eigen_model</span><span class="o">$</span><span class="n">vectors</span>

<span class="n">pc_manual</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="n">data_features</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">eigenvector</span>
</code></pre></div><p>Let&rsquo;s look at the scree plot:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="nf">tibble</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="nf">seq_along</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">),</span>  <span class="n">y</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_hline</span><span class="p">(</span><span class="n">yintercept</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span>
  <span class="nf">geom_line</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span> 
  <span class="nf">geom_point</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="m">3</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/pca-manual-scree-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>Looks identical to the previous one. Let&rsquo;s also look at the correlation
matrix between the principal components.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="nf">data.frame</span><span class="p">(</span><span class="n">pc_manual</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">cor</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">round</span><span class="p">(</span><span class="n">.,</span> <span class="m">4</span><span class="p">)</span>
</code></pre></div><pre><code>    X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 X15 X16 X17 X18 X19
X1   1  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
X2   0  1  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
X3   0  0  1  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
X4   0  0  0  1  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
X5   0  0  0  0  1  0  0  0  0   0   0   0   0   0   0   0   0   0   0
X6   0  0  0  0  0  1  0  0  0   0   0   0   0   0   0   0   0   0   0
X7   0  0  0  0  0  0  1  0  0   0   0   0   0   0   0   0   0   0   0
X8   0  0  0  0  0  0  0  1  0   0   0   0   0   0   0   0   0   0   0
X9   0  0  0  0  0  0  0  0  1   0   0   0   0   0   0   0   0   0   0
X10  0  0  0  0  0  0  0  0  0   1   0   0   0   0   0   0   0   0   0
X11  0  0  0  0  0  0  0  0  0   0   1   0   0   0   0   0   0   0   0
X12  0  0  0  0  0  0  0  0  0   0   0   1   0   0   0   0   0   0   0
X13  0  0  0  0  0  0  0  0  0   0   0   0   1   0   0   0   0   0   0
X14  0  0  0  0  0  0  0  0  0   0   0   0   0   1   0   0   0   0   0
X15  0  0  0  0  0  0  0  0  0   0   0   0   0   0   1   0   0   0   0
X16  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   1   0   0   0
X17  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   1   0   0
X18  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   1   0
X19  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   1
</code></pre>
<p>Yup, also still 0 across the board. Calculating a PCA by just using
matrix manipulations isn&rsquo;t too complicated, and it&rsquo;s a fun exercise, but
I&rsquo;d recommend just sticking to the <code>prcomp()</code> function, since it&rsquo;s a lot
simpler and offer better functionality. Now, let&rsquo;s move on to the
independent component analysis.</p>
<h2 id="independent-component-analysis">Independent Component Analysis</h2>
<p>While PCA attempts to find components explaining the maximum degree of
covariance or correlation, an ICA attemps to find components with
maximum statistical independence. There&rsquo;s very complicated nuance here
where PCA components are orthogonal and uncorrelated to each other and
ICA components are merely statistically independent, which is a very
subtle nuance. In practice, it&rsquo;ll mean that ICA components also have a
practically zero correlation. The main difference is in how the
components are obtained. Like with factor analysis, most ICA algorithms
require you to provide a number of components up front. The FastICA
algorithm we&rsquo;ll use here is a version of an ICA implementation. There&rsquo;s
also InfoMax and JADE to name two other implementations. I couldn&rsquo;t tell
you the difference between these ICA algorithms. The FastICA is
implemented in R through the <code>fastICA()</code> function and the eponymous
<code>{fastICA}</code> package (<a href="https://www.rdocumentation.org/packages/fastICA/versions/1.2-2/topics/fastICA">link to
documentation</a>).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">ica_model</span> <span class="o">&lt;-</span> <span class="nf">fastICA</span><span class="p">(</span><span class="n">data_features</span><span class="p">,</span> <span class="n">n.comp</span> <span class="o">=</span> <span class="n">n_comps</span><span class="p">)</span>
</code></pre></div><p>Let&rsquo;s create a weight matrix again. The output from the <code>fastICA()</code>
doesn&rsquo;t provide useful names, but the
<a href="https://www.rdocumentation.org/packages/fastICA/versions/1.2-2/topics/fastICA">documentation</a>
provides sufficient information. To create the weight matrix we take the
<code>A</code> field, transpose it, get the right names to the right places and
then create the plot like we&rsquo;ve done several times now.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">ica_weight_matrix</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">ica_model</span><span class="o">$</span><span class="n">A</span><span class="p">))</span> <span class="o">%&gt;%</span> 
  <span class="nf">rename_with</span><span class="p">(</span><span class="o">~</span> <span class="nf">str_glue</span><span class="p">(</span><span class="s">&#34;IC{seq(.)}&#34;</span><span class="p">))</span> <span class="o">%&gt;%</span>
  <span class="nf">mutate</span><span class="p">(</span><span class="n">variable</span> <span class="o">=</span> <span class="nf">names</span><span class="p">(</span><span class="n">data_features</span><span class="p">))</span> <span class="o">%&gt;%</span>
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="n">cols</span> <span class="o">=</span> <span class="nf">starts_with</span><span class="p">(</span><span class="s">&#34;IC&#34;</span><span class="p">),</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;ic&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;loading&#34;</span><span class="p">)</span>

<span class="n">ica_loading_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">ica_weight_matrix</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">ic</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">variable</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">loading</span><span class="p">))</span> <span class="o">+</span>
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;ICA loadings&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span> 
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;cork&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_fixed</span><span class="p">(</span><span class="n">ratio</span> <span class="o">=</span> <span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">ica_loading_plot</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/ica-weight-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>The FastICA method doesn&rsquo;t rank the components based on variance like
factor analysis and PCA. Degree of variance explained is not a part of
the FastICA algorithm. The ordering of the clusters is not meaningful,
we could label IC1 as IC6 and vice versa and the result would not be
less valid. So with that in mind, we can see that two of the components
(IC6 and IC4) are again very similar to the first two components we saw
in the factor analysis and the PCA. These appear to be very robust
components and will cluster together regardless of the method we use.
The direction of the components is also arbitrary, we could multiply the
loadings by -1 and they would not be less valid. One of the main
advantages of ICA in my opinion is that due to the statistical
independence, the components are easier to interpret and (often) have
clearer constructs than for instance principal components. For instance
IC7 consists of just a single item (&ldquo;behavior sexual risk&rdquo;), which the
algorithm determined was statistically independent from the other
variables. Interestingly, if we look back at the output from the factor
analysis, it says that &ldquo;behavior sexual risk&rdquo; scores highest for
&ldquo;uniqueness&rdquo; there as well. Also IC3 is most strongly powered by the
&ldquo;intention commitment&rdquo; variable, while also including some other
variables about intention and empowerment. This makes labeling
independent components (in my experience) a bit easier than principal
components.</p>
<p>Again, we can also visualize the correlation between the different
components.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">ica_model</span><span class="o">$</span><span class="n">S</span> <span class="o">%&gt;%</span> 
  <span class="nf">cor</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">data.frame</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;comp_x&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">pivot_longer</span><span class="p">(</span><span class="n">cols</span> <span class="o">=</span> <span class="nf">starts_with</span><span class="p">(</span><span class="s">&#34;X&#34;</span><span class="p">),</span> <span class="n">names_to</span> <span class="o">=</span> <span class="s">&#34;comp_y&#34;</span><span class="p">,</span> <span class="n">values_to</span> <span class="o">=</span> <span class="s">&#34;correlation&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">comp_x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">comp_y</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span>
  <span class="nf">geom_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">correlation</span><span class="p">,</span><span class="m">4</span><span class="p">)),</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;white&#34;</span><span class="p">)</span> <span class="o">+</span>
  <span class="nf">labs</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Correlation between ICs&#34;</span><span class="p">,</span>
       <span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span>
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;berlin&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">coord_equal</span><span class="p">()</span>
</code></pre></div><p><img src="index_files/figure-gfm/ica-corr-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>Again, the correlations between the components here are zero throughout,
which would fit with the statistical independence. While independent
components aren&rsquo;t forced to be uncorrelated, this is a feature of the
statistical independence. If the correlation matrix isn&rsquo;t zero
throughout, this would be a sign that you&rsquo;d need to adapt your ICA
method.</p>
<p>Okay, let&rsquo;s now compare the three approaches and put the loading
matrices side by side so we can look at the differences are bit more
closely. We&rsquo;ll take the three weight matrices from factor analysis, PCA,
and ICA, and bind them together. We&rsquo;ll then create a plot much in the
same way as before and we&rsquo;ll add a facet to show the weight matrices
side by side, separated by the method we used so it&rsquo;s easier to compare.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">all_weight_matrices</span> <span class="o">&lt;-</span> <span class="nf">bind_rows</span><span class="p">(</span>
  <span class="n">fa_weight_matrix</span> <span class="o">%&gt;%</span> 
    <span class="nf">rename</span><span class="p">(</span><span class="n">comp</span> <span class="o">=</span> <span class="n">factor</span><span class="p">)</span> <span class="o">%&gt;%</span> 
    <span class="nf">mutate</span><span class="p">(</span><span class="n">alg</span> <span class="o">=</span> <span class="s">&#34;FA&#34;</span><span class="p">),</span>
  <span class="n">pc_weight_matrix</span> <span class="o">%&gt;%</span> 
    <span class="nf">rename</span><span class="p">(</span><span class="n">comp</span> <span class="o">=</span> <span class="n">prin_comp</span><span class="p">)</span> <span class="o">%&gt;%</span> 
    <span class="nf">mutate</span><span class="p">(</span><span class="n">alg</span> <span class="o">=</span> <span class="s">&#34;PCA&#34;</span><span class="p">),</span> 
  <span class="n">ica_weight_matrix</span> <span class="o">%&gt;%</span> 
    <span class="nf">rename</span><span class="p">(</span><span class="n">comp</span> <span class="o">=</span> <span class="n">ic</span><span class="p">)</span> <span class="o">%&gt;%</span> 
    <span class="nf">mutate</span><span class="p">(</span><span class="n">alg</span> <span class="o">=</span> <span class="s">&#34;ICA&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">all_weight_matrices</span> <span class="o">%&gt;%</span> 
  <span class="nf">filter</span><span class="p">(</span><span class="nf">parse_number</span><span class="p">(</span><span class="n">comp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">n_comps</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">mutate</span><span class="p">(</span><span class="n">alg</span> <span class="o">=</span> <span class="nf">str_glue</span><span class="p">(</span><span class="s">&#34;{alg} loadings&#34;</span><span class="p">),</span>
         <span class="n">alg</span> <span class="o">=</span> <span class="nf">as_factor</span><span class="p">(</span><span class="n">alg</span><span class="p">))</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">comp</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">variable</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="n">loading</span><span class="p">))</span> <span class="o">+</span>
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">)</span> <span class="o">+</span> 
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;cork&#34;</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span> <span class="n">alg</span><span class="p">,</span> <span class="n">scales</span> <span class="o">=</span> <span class="s">&#34;free_x&#34;</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/all-weight-matrices-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:100.0%" /></p>
<p>Here we can most clearly see the overlap between the three methods,
Factor 1, PC1, and IC 6 capture essentially the same information. The
same goes for Factor 2, PC2, and IC 4. Other than that we can see that
the other components vary quite markedly. I wouldn&rsquo;t be comfortable
calling any of the other components &ldquo;fairly similar&rdquo; to another. You
could see how some variables load together with multiple methods, but
then the components those are captured in also have other information or
miss information. We already discussed IC7 consisting mostly of a single
variable, and a similar thing happens with Factor 4, but for a different
variable.</p>
<h3 id="bonus-hierarchical-clustering">BONUS: Hierarchical clustering</h3>
<p>I&rsquo;ll quickly go over hierarchical clustering too, it&rsquo;s simple and easy
to interpret. Hierarchical clustering works by taking your variables and
clustering them first into two groups, then three, then four, and so on.
It looks at similarity and a concept called &ldquo;Euclidian distance&rdquo; (other
methods are available) between the variables and determines how to
separate. Essentially, hierarchical clustering works by iteratively
separating variables into groups until every variable is on its own. It
does so rather aggressively, with the previous methods it&rsquo;s possible for
a variable to be part of two clusters, with hierarchical clustering it&rsquo;s
part of a single cluster only. This approach makes it an easy way to see
how variables cluster together at different thresholds.</p>
<p>First we convert our data frame with the features to a matrix, then we
transpose it. If we don&rsquo;t transpose, we&rsquo;d be clustering participants
together, and we&rsquo;re interested here in clustering of variables.
Clustering of individuals can be interesting but this post is already
long enough. Next we create a distance matrix using the <code>dist()</code>
function, this is a matrix containing the relative distance between the
variables to each other. You can think of the distance matrix as a sort
of weighted adjacency matrix. Then we can supply this distance matrix to
the hierarchical clustering function (<code>hclust()</code>,
<a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust">documentation</a>).
We don&rsquo;t need to supply the number of clusters at this stage.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">dist_data_features</span> <span class="o">&lt;-</span> <span class="nf">dist</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="nf">as.matrix</span><span class="p">(</span><span class="n">data_features</span><span class="p">)))</span>

<span class="n">hclust_model</span> <span class="o">&lt;-</span> <span class="nf">hclust</span><span class="p">(</span><span class="n">dist_data_features</span><span class="p">)</span>
</code></pre></div><p>We can visualize the hierarchical clustering with a dendrogram simply by
supplying the <code>hclust_model</code> to the <code>plot()</code> function (like
<code>plot(hclust_model)</code>), but in this case I&rsquo;d prefer to use the
<code>ggdendrogram()</code> function from the <code>{ggdendro}</code> package</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">ggdendro</span><span class="o">::</span><span class="nf">ggdendrogram</span><span class="p">(</span><span class="n">hclust_model</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/dendrogram-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>Here we can see the different &ldquo;branches&rdquo; of the tree. You can see that
for instance &ldquo;empowerment knowledge&rdquo; and &ldquo;empowerment abilities&rdquo; are
very close to each other, but it&rsquo;s a long way from &ldquo;perception severity&rdquo;
to &ldquo;motivation willingness&rdquo; for instance. You may also notice the values
along the y-axis. You could for instance &ldquo;cut&rdquo; the tree at <code>y = 13</code> and
you&rsquo;d separate the tree into 2 &ldquo;branches&rdquo;. You could cut it at
e.g. <code>y = 8</code>, and then you would get 11 clusters. Since we already
determined that we wanted 7 clusters, we can ask another function to
determine where to cut the tree to get the components we would like.
This is implemented in the <code>cutree()</code> function.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">hclust_weight_matrix</span> <span class="o">&lt;-</span> <span class="nf">cutree</span><span class="p">(</span><span class="n">hclust_model</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">n_comps</span><span class="p">)</span>
</code></pre></div><p>The <code>cutree()</code> function assigns a cluster to each of the variables. It
looks at the tree and determines where to cut the tree to get the
desired number of branches and then tells you the composition of the
branches. We can recreate this ourselves by simply adding a
<code>geom_hline()</code> to the dendrogram. With some trial and error, it seems
like cutting the tree at <code>y = 10.5</code> will give us 7 clusters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">ggdendro</span><span class="o">::</span><span class="nf">ggdendrogram</span><span class="p">(</span><span class="n">hclust_model</span><span class="p">)</span> <span class="o">+</span> 
  <span class="nf">geom_hline</span><span class="p">(</span><span class="n">yintercept</span> <span class="o">=</span> <span class="m">10.5</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;firebrick&#34;</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/dendrogram-w-line-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>Let&rsquo;s look at how the clusters are made up according to the hierarchical
clustering model. This isn&rsquo;t really a weight matrix, but rather a
definition of the clusters. The &ldquo;loadings&rdquo; here are not numerical, but
rather 1 or 0.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="n">hclust_weight_matrix</span> <span class="o">%&gt;%</span> 
  <span class="nf">data.frame</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="n">janitor</span><span class="o">::</span><span class="nf">clean_names</span><span class="p">()</span> <span class="o">%&gt;%</span> 
  <span class="nf">rename</span><span class="p">(</span><span class="n">cluster</span> <span class="o">=</span> <span class="n">x</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">rownames_to_column</span><span class="p">(</span><span class="s">&#34;variable&#34;</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="nf">as_factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">),</span> <span class="n">y</span> <span class="o">=</span> <span class="n">variable</span><span class="p">,</span> <span class="n">fill</span> <span class="o">=</span> <span class="nf">as_factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span> <span class="o">+</span>
  <span class="nf">geom_tile</span><span class="p">()</span> <span class="o">+</span> 
  <span class="nf">labs</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">y</span> <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span>
       <span class="n">fill</span> <span class="o">=</span> <span class="s">&#34;cluster&#34;</span><span class="p">)</span> <span class="o">+</span> 
  <span class="n">scico</span><span class="o">::</span><span class="nf">scale_fill_scico_d</span><span class="p">(</span><span class="n">palette</span> <span class="o">=</span> <span class="s">&#34;batlow&#34;</span><span class="p">)</span>
</code></pre></div><p><img src="index_files/figure-gfm/hclust-weight-matrix-1.png"
data-fig-align="center" style="display:block; margin:auto;"
style="width:80.0%" /></p>
<p>We can again see that the same two clusters show up. Cluster 6 and 7
resemble those of Factor 2 and Factor 1, PC2 and PC1, and IC4 and IC6
respectively. If you want to learn more about what you can do with the
<code>hlcust()</code> and associated functions, you can check out <a href="https://uc-r.github.io/hc_clustering">this
webpage</a>.</p>
<p>Okay I&rsquo;d like to leave it at that. This blogpost is long enough. Again,
I had to simplify and take some shortcuts, if you think I made mistakes
in that effort, please let me know and I&rsquo;ll fix it as well as I can!</p>

  </article>

  <br/>

  
  
</section>

      </div>
      
        
<footer class="footer">
  <script src="https://yihui.org/js/math-code.js"></script>
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js"></script>
</footer>
      
    </main>

    

  <script src="/js/app.js"></script>
  <script>$("img").parents('p').css("text-align", "center");</script>

  
  </body>
</html>
